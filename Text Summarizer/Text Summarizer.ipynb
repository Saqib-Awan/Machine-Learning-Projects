{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e1eeee",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7a3f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Computer\n",
      "[nltk_data]     Arena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Computer\n",
      "[nltk_data]     Arena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from heapq import nlargest\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b7cdd",
   "metadata": {},
   "source": [
    "### Download necessary NLTK data if not already downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd18986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Computer\n",
      "[nltk_data]     Arena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Computer\n",
      "[nltk_data]     Arena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from heapq import nlargest\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee14eb2",
   "metadata": {},
   "source": [
    "### Defining the function to take the input from the user and summarize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60bcb9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the text to summarize: This script uses the YAKE library to perform keyword extraction from user-provided text. After importing YAKE and creating a keyword extractor, it prompts the user to input text and then extracts and displays the most relevant keywords along with their scores. This tool helps identify key phrases and terms within a given piece of content.\n",
      "\n",
      "Summary:\n",
      "After importing YAKE and creating a keyword extractor, it prompts the user to input text and then extracts and displays the most relevant keywords along with their scores.\n"
     ]
    }
   ],
   "source": [
    "def extract_summary_from_user_input():\n",
    "    # Take input from the user\n",
    "    text = input(\"Please enter the text to summarize: \")\n",
    "    \n",
    "    # Determine the number of sentences to include in the summary\n",
    "    if text.count(\". \") > 20:\n",
    "        length = int(round(text.count(\". \")/10, 0))\n",
    "    else:\n",
    "        length = 1\n",
    "\n",
    "    # Remove punctuation\n",
    "    nopuch = [char for char in text if char not in string.punctuation]\n",
    "    nopuch = \"\".join(nopuch)\n",
    "\n",
    "    # Remove stop words\n",
    "    processed_text = [word for word in nopuch.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freq = {}\n",
    "    for word in processed_text:\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 1\n",
    "        else:\n",
    "            word_freq[word] = word_freq[word] + 1\n",
    "\n",
    "    max_freq = max(word_freq.values())\n",
    "    for word in word_freq.keys():\n",
    "        word_freq[word] = (word_freq[word]/max_freq)\n",
    "\n",
    "    # Score sentences\n",
    "    sent_list = nltk.sent_tokenize(text)\n",
    "    sent_score = {}\n",
    "    for sent in sent_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_freq.keys():\n",
    "                if sent not in sent_score.keys():\n",
    "                    sent_score[sent] = word_freq[word]\n",
    "                else:\n",
    "                    sent_score[sent] = sent_score[sent] + word_freq[word]\n",
    "\n",
    "    # Generate summary\n",
    "    summary_sents = nlargest(length, sent_score, key=sent_score.get)\n",
    "    summary = \" \".join(summary_sents)\n",
    "    \n",
    "    # Print the summary\n",
    "    print(\"\\nSummary:\")\n",
    "    print(summary)\n",
    "\n",
    "# Run the function\n",
    "extract_summary_from_user_input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
